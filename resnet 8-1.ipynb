{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from medpy.io import load\n",
    "from skimage.measure import block_reduce\n",
    "import scipy\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import nilearn as nb\n",
    "from nilearn import masking\n",
    "import nibabel as nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input_resnet = tf.keras.applications.resnet.preprocess_input\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = sitk.ReadImage(os.getcwd() + \"/doi_10_5061_dryad_5jv56__v20160424/Kim.2015.PLOS1.OCD-mm.data/mCCA+jICA_1K_results/HO1K_joint_comp_ica_feature_1_001.hdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_array = sitk.GetArrayFromImage(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(t1_array[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "91/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    arr = arr + abs(np.amin(arr))\n",
    "    assert np.amax(arr) != 0\n",
    "    arr = arr / np.amax(arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "name = []\n",
    "\n",
    "for i in range(500):\n",
    "    try :\n",
    "        t1 = sitk.ReadImage(os.getcwd() + \"/Data/FA\" + str(1000+i) + \".nii\")\n",
    "        t2 = sitk.GetArrayFromImage(t1)\n",
    "        t3 = []\n",
    "        for j in range(0,90):\n",
    "            result = scipy.ndimage.zoom(t2[j], 224/288)\n",
    "            t3.append(result)\n",
    "        X.append(t3)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "for i in range(500):\n",
    "    try :\n",
    "        t1 = sitk.ReadImage(os.getcwd() + \"/Data/FA\" + str(2000+i) + \".nii\")\n",
    "        t2 = sitk.GetArrayFromImage(t1)\n",
    "        t3 = []\n",
    "        for j in range(0,90):\n",
    "            result = scipy.ndimage.zoom(t2[j], 224/288)\n",
    "            t3.append(result)\n",
    "        X.append(t3)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(len(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0] = np.array(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "# 2\n",
    "# (72, 40, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    arr = arr + abs(np.amin(arr))\n",
    "    assert np.amax(arr) != 0\n",
    "    arr = arr / np.amax(arr)\n",
    "    return arr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate(( [0]*34, [1]*30 ))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=34)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.resize(X_train, ((40*44), 85, 71))\n",
    "X_test = np.resize(X_test, ((40*20), 85, 71))\n",
    "\n",
    "temp_y_train = np.array([])\n",
    "for i in range(len(y_train)):\n",
    "    for j in range(40):\n",
    "        temp_y_train = np.append(temp_y_train, y_train[i])\n",
    "\n",
    "temp_y_test = np.array([])\n",
    "for i in range(len(y_test)):\n",
    "    for j in range(40):\n",
    "        temp_y_test = np.append(temp_y_test, y_test[i])\n",
    "\n",
    "y_train = temp_y_train\n",
    "y_test = temp_y_test\n",
    "\n",
    "print(np.shape(X_train), np.shape(y_train), np.shape(X_test), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_RGB = np.stack((X_train,)*3, axis=-1)\n",
    "\n",
    "X_test_RGB = np.stack((X_test,)*3, axis=-1)\n",
    "\n",
    "train_ds = preprocess_input_resnet(X_train_RGB)\n",
    "test_ds = preprocess_input_resnet(X_test_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_resnet = tf.keras.applications.ResNet50(input_shape=(85, 71, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model_resnet.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input((85, 71, 3))\n",
    "x = base_model_resnet(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "predictions = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_resnet = tf.keras.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model_resnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_resnet.fit(train_ds, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(test_ds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_resnet.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model_resnet.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 10\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model_resnet.layers[:fine_tune_at]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  10 + fine_tune_epochs\n",
    "\n",
    "history_fine = model_resnet.fit(train_ds, y_train,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=(test_ds, y_test), callbacks=[\n",
    "                         tf.keras.callbacks.EarlyStopping(\n",
    "                         monitor='val_loss',\n",
    "                         patience=12,\n",
    "                         mode='auto',\n",
    "                         restore_best_weights=True,\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n",
    "\n",
    "initial_epochs = 10\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.plot([initial_epochs,initial_epochs],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs,initial_epochs],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = y_test\n",
    "# for i in X_test: print(i[:3000], \"\\n\")\n",
    "\n",
    "print(y_true)\n",
    "\n",
    "for i in test_ds:\n",
    "    i = np.expand_dims(i, axis=0)\n",
    "    print(model_resnet.predict(i))\n",
    "    j = np.round(model_resnet.predict(i))\n",
    "    # print(j)\n",
    "    y_pred.append((np.round(model_resnet.predict(i))))\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct_labels = np.array(tf.concat([item for item in y_true], axis = 0))\n",
    "predicted_labels = np.array(tf.concat([item for item in y_pred], axis = 0))\n",
    "\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, predicted_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=2)\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=[\"Healthy\", \"Depression\"],\n",
    "            yticklabels=[\"Healthy\", \"Depression\"],\n",
    "            annot=True, fmt='g', annot_kws={\"size\":40})\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n",
    "\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.save('RESNET_ocd_resting_state_dataset_t1_2d.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c35cd825f79867027b51bf887e34ee2ad9eef7e3e25607ce545dc148c5ab8a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
